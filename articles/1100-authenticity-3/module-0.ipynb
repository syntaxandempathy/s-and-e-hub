{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sOjkNR_t-67"
      },
      "outputs": [],
      "source": [
        "# cell 0.0: foundations: helpers (no installs, no heavy imports)\n",
        "from __future__ import annotations\n",
        "\n",
        "import os, sys, json, random, re, unicodedata, datetime, textwrap, math, subprocess\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "def _pip(*args: str) -> None:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", *args])\n",
        "\n",
        "def try_import(name: str, import_name: Optional[str] = None):\n",
        "    try:\n",
        "        return __import__(import_name or name)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def ensure_packaging():\n",
        "    try:\n",
        "        from packaging.version import Version  # noqa: F401\n",
        "        from packaging.specifiers import SpecifierSet  # noqa: F401\n",
        "    except Exception:\n",
        "        _pip(\"install\", \"-q\", \"--no-cache-dir\", \"packaging\")\n",
        "\n",
        "def versions_dict(pkgs=(\"numpy\",\"pandas\",\"matplotlib\",\"scipy\",\"sklearn\")) -> Dict[str, Optional[str]]:\n",
        "    out = {}\n",
        "    for p in pkgs:\n",
        "        mod = try_import(p if p!=\"sklearn\" else \"sklearn\")\n",
        "        out[p] = getattr(mod, \"__version__\", None) if mod else None\n",
        "    return out\n",
        "\n",
        "print(json.dumps({\"cell_id\":\"0.0 foundations: helpers\",\"status\":\"ready\"}, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    installed = {}\n",
        "    order = [\"numpy\", \"scipy\", \"scikit-learn\", \"pandas\", \"matplotlib\"]\n",
        "    specs = {\n",
        "        \"numpy\": \">=2.0,<3\",\n",
        "        \"pandas\": \"==2.2.2\",\n",
        "        \"matplotlib\": \">=3.8,<3.9\",\n",
        "        \"scipy\": \">=1.13,<2.0\",\n",
        "        \"scikit-learn\": \">=1.6,<1.7\",\n",
        "    }\n",
        "    for pip_name in order:\n",
        "        import_name = \"sklearn\" if pip_name == \"scikit-learn\" else pip_name\n",
        "        if try_import(import_name) is None:\n",
        "            _pip(\"install\", \"-q\", \"--no-cache-dir\", \"--only-binary=:all:\", f\"{pip_name}{specs[pip_name]}\")\n",
        "            installed[pip_name] = specs[pip_name]\n",
        "    print(json.dumps({\n",
        "        \"cell_id\":\"0.1 foundations: installs (only-if-missing)\",\n",
        "        \"status\":\"pass\",\"installed_if_missing\": installed,\"versions\": versions_dict()\n",
        "    }, indent=2))\n",
        "except Exception as e:\n",
        "    print(json.dumps({\"cell_id\":\"0.1 foundations: installs (only-if-missing)\",\n",
        "                      \"status\":\"fail\",\"error\":f\"{type(e).__name__}: {e}\"} , indent=2))\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "XX0SzGtnuMgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.2: foundations: imports, determinism, version capture (CPU-only)\n",
        "\n",
        "# stdlib (import locally so this cell is idempotent)\n",
        "import os, sys, json, random\n",
        "\n",
        "# Cap threads BEFORE importing numeric libs (repro & predictable perf)\n",
        "threads = os.environ.get(\"LSA_THREADS\", \"1\")\n",
        "for _v in (\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\",\"OMP_NUM_THREADS\"):\n",
        "    os.environ[_v] = threads\n",
        "\n",
        "# heavy stack (single imports cell)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import sklearn\n",
        "\n",
        "# Allow thread override; default to 1 for reproducibility on CPU\n",
        "threads = os.environ.get(\"LSA_THREADS\", \"1\")\n",
        "for _v in (\"OPENBLAS_NUM_THREADS\",\"MKL_NUM_THREADS\",\"NUMEXPR_NUM_THREADS\",\"OMP_NUM_THREADS\"):\n",
        "    os.environ[_v] = threads\n",
        "\n",
        "SEED = int(os.environ.get(\"LSA_SEED\",\"12345\"))\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)  # note: only affects new processesâ€™ hash order\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "\n",
        "# Optional strict hash guard (off by default)\n",
        "if os.environ.get(\"LSA_HASH_GUARD\",\"0\") == \"1\":\n",
        "    import builtins\n",
        "    _orig_hash = builtins.hash\n",
        "    def _det_hash_guard(x):\n",
        "        raise RuntimeError(\"Avoid builtins.hash() for logic; nondeterministic across runs.\")\n",
        "    builtins.hash = _det_hash_guard\n",
        "\n",
        "# Plot defaults (matplotlib per your preference)\n",
        "plt.rcParams.update({\"figure.figsize\": (8,3.5), \"axes.grid\": True, \"grid.alpha\": 0.25, \"figure.dpi\": 110})\n",
        "\n",
        "VERSIONS = {\n",
        "    \"python\": sys.version.split()[0],\n",
        "    \"numpy\": np.__version__,\n",
        "    \"pandas\": pd.__version__,\n",
        "    \"matplotlib\": matplotlib.__version__,\n",
        "    \"scipy\": scipy.__version__,\n",
        "    \"scikit_learn\": sklearn.__version__,\n",
        "}\n",
        "print(json.dumps({\"cell_id\":\"0.2 foundations: imports+determinism\",\"status\":\"pass\",\"seed\":SEED,\"threads\":threads,\"versions\":VERSIONS}, indent=2))\n"
      ],
      "metadata": {
        "id": "M8dqQhvhuVP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.3: foundations: configuration & directories (portable)\n",
        "BASE_DIR = Path(os.environ.get(\"LSA_BASE_DIR\",\"/content\"))\n",
        "\n",
        "@dataclass\n",
        "class LSAPaths:\n",
        "    input_dir: Path = BASE_DIR / \"input_docs\"\n",
        "    out_dir: Path = BASE_DIR / \"lsa_outputs\"\n",
        "    models_dir: Path = BASE_DIR / \"models\"\n",
        "\n",
        "@dataclass\n",
        "class LSAConfig:\n",
        "    language: str = \"en\"\n",
        "    window_sentences: int = 4\n",
        "    window_stride: int = 2\n",
        "    percentile_minor: float = 0.60\n",
        "    percentile_unchanged: float = 0.85\n",
        "    new_floor: float = 0.20\n",
        "    new_floor_tfidf: float = 0.15\n",
        "\n",
        "paths = LSAPaths(); cfg = LSAConfig()\n",
        "for p in (paths.input_dir, paths.out_dir, paths.models_dir):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(json.dumps({\n",
        "    \"cell_id\":\"0.3 foundations: config+dirs\",\n",
        "    \"status\":\"pass\",\n",
        "    \"base_dir\": str(BASE_DIR),\n",
        "    \"dirs\": { \"input\": str(paths.input_dir), \"out\": str(paths.out_dir), \"models\": str(paths.models_dir) }\n",
        "}, indent=2))\n"
      ],
      "metadata": {
        "id": "ubx1OKhrvGxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.4: foundations: status logging & self-report\n",
        "MODULE_STATUS: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "def report_status(module: str, ok: bool, note: str = \"\", extra: Optional[Dict[str,Any]] = None):\n",
        "    MODULE_STATUS[module] = {\n",
        "        \"ok\": bool(ok),\n",
        "        \"note\": note,\n",
        "        \"extra\": extra or {},\n",
        "        \"ts\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "    }\n",
        "    print(json.dumps({\"module\": module, \"ok\": ok, \"note\": note, \"extra\": extra or {} }))\n",
        "\n",
        "def dump_status_json(out_path: Path = paths.out_dir / \"module_status.json\"):\n",
        "    out_path.write_text(json.dumps(MODULE_STATUS, indent=2), encoding=\"utf-8\")\n",
        "    print(json.dumps({\"wrote\": str(out_path), \"entries\": len(MODULE_STATUS)}))\n",
        "\n",
        "# cell-level self-report (per rule #9)\n",
        "print(json.dumps({\"cell_id\": \"0.4 foundations: status utilities\", \"status\": \"pass\"}))\n"
      ],
      "metadata": {
        "id": "6Z6DBSDZvIBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.5: foundations: offset-preserving normalization (fixed CRLF + maps)\n",
        "from typing import NamedTuple\n",
        "\n",
        "class NormResult(NamedTuple):\n",
        "    text: str\n",
        "    norm_to_orig: List[int]\n",
        "    orig_to_norm: List[int]\n",
        "\n",
        "def normalize_with_offsets(s: str) -> NormResult:\n",
        "    norm_to_orig: List[int] = []\n",
        "    out_chars: List[str] = []\n",
        "    prev_space = False\n",
        "    j = 0\n",
        "    L = len(s)\n",
        "\n",
        "    while j < L:\n",
        "        ch = s[j]\n",
        "        if ch == '\\r':\n",
        "            if j + 1 < L and s[j+1] == '\\n':\n",
        "                out_chars.append('\\n'); norm_to_orig.append(j)\n",
        "                prev_space = False; j += 2; continue\n",
        "            else:\n",
        "                ch_n = '\\n'\n",
        "        else:\n",
        "            ch_n = unicodedata.normalize(\"NFKC\", ch)\n",
        "            if ch_n in [\"\\u2028\", \"\\u2029\"]:\n",
        "                ch_n = \"\\n\"\n",
        "\n",
        "        if ch_n.isspace() and ch_n != \"\\n\":\n",
        "            if prev_space: j += 1; continue\n",
        "            ch_n = \" \"; prev_space = True\n",
        "        else:\n",
        "            prev_space = (ch_n == \" \")\n",
        "\n",
        "        out_chars.append(ch_n); norm_to_orig.append(j); j += 1\n",
        "\n",
        "    text = \"\".join(out_chars)\n",
        "    orig_to_norm = [-1] * L\n",
        "    for norm_i, orig_j in enumerate(norm_to_orig):\n",
        "        if 0 <= orig_j < L:\n",
        "            orig_to_norm[orig_j] = norm_i\n",
        "    last = 0\n",
        "    for k in range(L):\n",
        "        if orig_to_norm[k] == -1:\n",
        "            orig_to_norm[k] = last\n",
        "        else:\n",
        "            last = orig_to_norm[k]\n",
        "\n",
        "    return NormResult(text=text, norm_to_orig=norm_to_orig, orig_to_norm=orig_to_norm)\n",
        "\n",
        "# sanity self-report\n",
        "try:\n",
        "    demo = \"A  test\\u00A0string\\r\\nwith\\rmixed  spaces.\\nNext line.\"\n",
        "    res = normalize_with_offsets(demo)\n",
        "    assert \"\\r\" not in res.text and \"\\r\\n\" not in res.text\n",
        "    report_status(\"0.foundation.normalize\", True, \"Normalization OK\", {\"len\": len(res.text)})\n",
        "except Exception as e:\n",
        "    report_status(\"0.foundation.normalize\", False, f\"Init error: {e}\")\n"
      ],
      "metadata": {
        "id": "C_LN4zeRvJ4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.6: foundations: sentence segmentation & windowing (regex heuristic)\n",
        "# Reviewer fix: reduce false splits on initials like \"A.\" / \"Dr.\"\n",
        "SENT_SPLIT_RE = re.compile(r'(?<!\\b[A-Z]\\.)(?<=[.!?])\\s+(?=[A-Z0-9])')\n",
        "\n",
        "def split_sentences(text: str) -> List[Tuple[str, Tuple[int,int]]]:\n",
        "    spans: List[Tuple[int,int]] = []\n",
        "    start = 0\n",
        "    for m in SENT_SPLIT_RE.finditer(text):\n",
        "        end = m.start() + 1\n",
        "        spans.append((start, end))\n",
        "        start = m.end()\n",
        "    if start < len(text): spans.append((start, len(text)))\n",
        "    return [(text[a:b], (a,b)) for a,b in spans if b > a]\n",
        "\n",
        "def window_sentences(sents: List[Tuple[str, Tuple[int,int]]], win: int, stride: int):\n",
        "    windows = []\n",
        "    for i in range(0, max(len(sents)-win+1, 0), stride):\n",
        "        chunk = sents[i:i+win]\n",
        "        if len(chunk) < win: break\n",
        "        text = \" \".join(x[0].strip() for x in chunk)\n",
        "        start = chunk[0][1][0]; end = chunk[-1][1][1]\n",
        "        windows.append({\"sent_start_idx\": i, \"sent_end_idx\": i+win, \"char_span\": [start,end], \"text\": text})\n",
        "    return windows\n",
        "\n",
        "# sanity self-report\n",
        "try:\n",
        "    sample = \"Dr. A. Smith wrote this. Second sentence! Third sentence? Fourth one. Fifth here.\"\n",
        "    sents = split_sentences(sample)\n",
        "    wins = window_sentences(sents, 4, 2)\n",
        "    report_status(\"0.foundation.segmentation\", True, \"Splitter/windowing OK\", {\"sents\": len(sents), \"windows\": len(wins)})\n",
        "except Exception as e:\n",
        "    report_status(\"0.foundation.segmentation\", False, f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "u49mIDpuvlUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cell 0.7: foundations: visualization smoke test (matplotlib only) â€” use plt from 0.2\n",
        "try:\n",
        "    xs = np.arange(0, 10); ys = np.sqrt(xs)\n",
        "    plt.figure(); plt.plot(xs, ys, marker=\"o\")\n",
        "    plt.title(\"Matplotlib render check (0.#)\"); plt.xlabel(\"x\"); plt.ylabel(\"sqrt(x)\")\n",
        "    fig = plt.gcf(); axes = fig.get_axes()\n",
        "    assert axes and any(ax.has_data() for ax in axes), \"No plotted data detected\"\n",
        "    plt.show()\n",
        "    report_status(\"0.foundation.viz\", True, \"Matplotlib rendering OK\")\n",
        "except Exception as e:\n",
        "    report_status(\"0.foundation.viz\", False, f\"Matplotlib failed: {e}\")\n"
      ],
      "metadata": {
        "id": "_qNtVT1dv5z4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}