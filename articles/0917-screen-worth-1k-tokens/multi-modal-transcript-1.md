Historically, I’ve used screenshots to share designs with other designers, stakeholders, or vendors. I’ve also used them to document my own work in my career diary. They’ve mainly been a way to communicate finished designs, or key screens for approval before moving on to full prototypes.

Before tools like Figma, you’d get screenshots approved and then design only what was needed for development. About 20 years ago, we’d do one or two full screens, get approval, and then hack up the HTML. We wouldn’t do every screen—that would’ve been too time-consuming. Usually, we’d produce a homepage, a landing page, and maybe one or two other layouts. Once those were approved, we’d go straight to HTML.

As time progressed, tools improved, and we started doing more screenshots because people liked seeing them. Fast forward to today, with tools like Sketch and Figma, we can build clickable prototypes and complete workflows. But at the end of the day, they’re still essentially screenshots wired together to communicate design.

As I mentioned, I also use screenshots to document my own work over time, should I need them for future opportunities.

Now, with AI’s multimodal capabilities, a whole new world has opened up. I can use screenshots to communicate things like information architecture. For example, I can provide screenshots of an entire workflow and ask the AI to infer the IA. For simple workflows, it works well. For complex ones—like a cloud service creation flow—it might be 80% accurate. But even that saves me days, possibly a week, of work.

It can even identify text labels and generate site maps, although many people confuse site maps with IA, which is frustrating.

Recently, I’ve been working on the latest version of my transparency tool. I’m using multiple sessions to mirror agent behavior. When I get outputs from the notebook, I can screenshot them and share with an “expert” AI, which then reviews the graphs and points out corrections. I give that feedback to the developer, then the coder implements the fix.

Screenshots also help in other ways. For example, if I run Python code and get an error, I can screenshot it, feed it to the AI, and get a fix immediately. It’s often faster than copy-pasting or typing.

All of this shows that screenshots are becoming part of the future of multimodal workflows. Eventually, we’ll probably start thinking more intentionally about capturing them in ways that yield the best results. That’s the next question.